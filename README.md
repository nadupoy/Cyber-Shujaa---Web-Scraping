# ğŸ•¸ï¸ Cyber Shujaa - Web Scraping ğŸš€

This is my updated solution to the week 1 assignment in the Cyber Shujaa program 
for the **Data and AI Specialist** track.

## ğŸ§­ Table of contents

- [ğŸŒŸ Overview](#ğŸŒŸ-overview)
  - [The assignment ğŸ¯](#the-assignment-ğŸ¯)
  - [Links ğŸ”—](#links-ğŸ”—)
- [ğŸ› ï¸ My process](#ğŸ› ï¸-my-process)
  - [Built with ğŸ§±](#built-with-ğŸ§±)
  - [What I learned ğŸ§ ](#what-i-learned-ğŸ§ )
  - [Continued development ğŸŒ±](#continued-development-ğŸŒ±)
  - [Useful resources ğŸ“š](#useful-resources-ğŸ“š)
- [ğŸ‘©â€ğŸ’» Author](#ğŸ‘©â€ğŸ’»-author)

## ğŸŒŸ Overview

### The assignment ğŸ¯

The goal of the assignment was to develop hands-on experience gathering web data 
using **Python**.

The objectives were:
  1. Practical Python coding ğŸ
  2. Data extraction from a web page ğŸ•¸ï¸
  3. Parsing and cleaning the extracted data âœ¨
  4. Storing structured data into a **Pandas DataFrame** ğŸ“Š
  5. Export the final dataset into a `.csv` file ğŸ’¾

### Links ğŸ”—

- [Google Colab](https://colab.research.google.com/drive/1plbhwdRXuqQXqgncfocjvGL6ntMn_eVj?usp=sharing) 
assignment submission

## ğŸ› ï¸ My process

### Built with ğŸ§±

- **[Python](https://www.python.org)** ğŸ
- **[Requests](https://requests.readthedocs.io/en/latest)** HTTP library for Python ğŸ“¡
- **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc)** Python 
library for pulling data out of HTML and XML files ğŸ§º
- **[Pandas](https://pandas.pydata.org/docs/index.html)** Python library used for 
working with data sets with functions for cleaning, exploring and manipulating data ğŸ¼

### What I learned ğŸ§ 

**1. Making requests using the `requests` library** ğŸ“©

```python
    url = "https://www.scrapethissite.com/pages/forms"
    response = requests.get(url)
```

**2. Parsing html using the `BeautifulSoup` library** ğŸ“

```python
    html_doc = BeautifulSoup(response.text, "html.parser")
```

**3. Selecting and scraping data from `html` elements using tag names and class names** ğŸ”

This was done by using `find_all()` method in `BeautifulSoup`.

Using tag names only:

```python
    # combine all table headers into a Python list:
    th = html_doc.find_all("th")
```

Using class names:
```python
    # combine all team names into a Python list:
    td_name = html_doc.find_all("td", class_="name")
```

**4. Scraping a paginated website** ğŸ¤¯

This was the **biggest challenge** I faced while doing this assignment.

I managed to solve it by using a `while` loop and an `if...else` statement. ğŸ’¡

```python
    # while loop to iterate over all the pages:
    i = 1
    while i < 25:
      ...
      # if...else statement to load the DataFrame in page 01 and join all the DataFrames from each webpage into one DataFrame:
      if i == 1:
        df = pd.DataFrame(data)
      else:
        # combine subsequent tables:
        df_concat = pd.DataFrame(data)
        df = pd.concat([df, df_concat], axis=0)
```

**5. Creating `Pandas` `Series` and `DataFrames`** ğŸ“Š

This was done using a Python object as outlined in the `Pandas` documentation and 
tutorials online.

An item in the object forms a `Pandas` `Series` i.e. table columns, with the keys 
forming the table headers and values, which are in the form of a Python list, form 
the table values associated with the table headers.

```python
    data = {
      table_headers[0]: team_names,
      ...
      table_headers[8]: goal_differences
    }
```

When `Series` are combined together in a Python object they are used to create a 
`Pandas` `DataFrame`.
```python
    df = pd.DataFrame(data)
```

### Continued development ğŸŒ±

This assignment gave me a hands-on, practical introduction to web scraping and the 
library used. Not only did I learn the *what* but also the *why* of each step and 
library in the web scraping process.

I am going to learn more about:

- `requests` library. I assume it is also applicable in fetching data from not only 
web servers but other servers as well. ğŸŒ
- `Pandas` ğŸ“ˆ
- `BeautifulSoup` ğŸ“š

### Useful resources ğŸ“š

- [Combining `DataFrames`](https://pandas.pydata.org/docs/getting_started/intro_tutorials/08_combine_dataframes.html#) 
in `Pandas`
- [Web scraping](https://realpython.com/python-web-scraping-practical-introduction) 
in Python
- `Pandas` [`Series`](https://www.w3schools.com/python/pandas/pandas_series.asp)
- `Pandas` [`DataFrames`](https://www.w3schools.com/python/pandas/pandas_dataframes.asp)

## ğŸ‘©â€ğŸ’» Author

- LinkedIn - [Grace Sampao](https://www.linkedin.com/in/grace-sampao)
- GitHub - [@nadupoy](https://github.com/nadupoy)
